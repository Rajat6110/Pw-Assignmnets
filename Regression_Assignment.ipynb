{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression Analysis Questions (PwSkills)\n",
        "\n",
        "## Simple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. What is Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Linear Regression is a statistical method that models the relationship between \n",
        "# a dependent variable (Y) and a single independent variable (X) using a straight line.\n",
        "# It assumes a linear relationship: Y = mX + c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. What are the key assumptions of Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nKey assumptions of Simple Linear Regression:\\n1. Linearity: The relationship between X and Y is linear\\n2. Independence: Observations are independent of each other\\n3. Homoscedasticity: Constant variance of residuals\\n4. Normality: Residuals are normally distributed\\n5. No outliers: Extreme values don't unduly influence the model\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Key assumptions of Simple Linear Regression:\n",
        "1. Linearity: The relationship between X and Y is linear\n",
        "2. Independence: Observations are independent of each other\n",
        "3. Homoscedasticity: Constant variance of residuals\n",
        "4. Normality: Residuals are normally distributed\n",
        "5. No outliers: Extreme values don't unduly influence the model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. What does the coefficient m represent in the equation Y=mX+c?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The coefficient 'm' represents the slope of the regression line.\n",
        "# It indicates the change in Y for a one-unit change in X.\n",
        "# If m > 0: positive relationship, if m < 0: negative relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. What does the intercept c represent in the equation Y=mX+c?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The intercept 'c' represents the value of Y when X equals zero.\n",
        "# It's the point where the regression line crosses the Y-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. How do we calculate the slope m in Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe slope m is calculated using the formula:\\nm = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]\\n\\nWhere:\\n- xi, yi are individual data points\\n- x̄, ȳ are the means of X and Y respectively\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The slope m is calculated using the formula:\n",
        "m = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]\n",
        "\n",
        "Where:\n",
        "- xi, yi are individual data points\n",
        "- x̄, ȳ are the means of X and Y respectively\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. What is the purpose of the least squares method in Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe least squares method finds the best-fitting line by minimizing the sum of squared\\nresiduals (differences between observed and predicted values). This ensures the line\\nthat best represents the data with minimum prediction error.\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The least squares method finds the best-fitting line by minimizing the sum of squared\n",
        "residuals (differences between observed and predicted values). This ensures the line\n",
        "that best represents the data with minimum prediction error.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nR² represents the proportion of variance in the dependent variable that is predictable\\nfrom the independent variable. It ranges from 0 to 1:\\n- R² = 0: No linear relationship\\n- R² = 1: Perfect linear relationship\\n- R² = 0.7 means 70% of variance in Y is explained by X\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "R² represents the proportion of variance in the dependent variable that is predictable\n",
        "from the independent variable. It ranges from 0 to 1:\n",
        "- R² = 0: No linear relationship\n",
        "- R² = 1: Perfect linear relationship\n",
        "- R² = 0.7 means 70% of variance in Y is explained by X\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. What is Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nMultiple Linear Regression extends simple linear regression to include multiple \\nindependent variables. The equation is:\\nY = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε\\n\\nWhere Y is dependent variable, X₁, X₂, ..., Xₙ are independent variables,\\nand β values are coefficients.\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Multiple Linear Regression extends simple linear regression to include multiple \n",
        "independent variables. The equation is:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε\n",
        "\n",
        "Where Y is dependent variable, X₁, X₂, ..., Xₙ are independent variables,\n",
        "and β values are coefficients.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. What is the main difference between Simple and Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Linear Regression uses one independent variable (Y = mX + c)\n",
        "# Multiple Linear Regression uses multiple independent variables (Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. What are the key assumptions of Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nKey assumptions of Multiple Linear Regression:\\n1. Linearity: Linear relationship between dependent and independent variables\\n2. Independence: Observations are independent\\n3. Homoscedasticity: Constant variance of residuals\\n4. Normality: Residuals are normally distributed\\n5. No multicollinearity: Independent variables are not highly correlated\\n6. No autocorrelation: Residuals are not correlated with each other\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Key assumptions of Multiple Linear Regression:\n",
        "1. Linearity: Linear relationship between dependent and independent variables\n",
        "2. Independence: Observations are independent\n",
        "3. Homoscedasticity: Constant variance of residuals\n",
        "4. Normality: Residuals are normally distributed\n",
        "5. No multicollinearity: Independent variables are not highly correlated\n",
        "6. No autocorrelation: Residuals are not correlated with each other\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nHeteroscedasticity occurs when the variance of residuals is not constant across\\nall levels of independent variables. Effects:\\n- Makes standard errors unreliable\\n- Affects confidence intervals and hypothesis tests\\n- Can lead to inefficient parameter estimates\\n- Violates the homoscedasticity assumption\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across\n",
        "all levels of independent variables. Effects:\n",
        "- Makes standard errors unreliable\n",
        "- Affects confidence intervals and hypothesis tests\n",
        "- Can lead to inefficient parameter estimates\n",
        "- Violates the homoscedasticity assumption\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nMethods to handle multicollinearity:\\n1. Remove highly correlated variables\\n2. Use Ridge or Lasso regression\\n3. Principal Component Analysis (PCA)\\n4. Combine correlated variables into composite variables\\n5. Increase sample size\\n6. Use Variance Inflation Factor (VIF) to identify problematic variables\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Methods to handle multicollinearity:\n",
        "1. Remove highly correlated variables\n",
        "2. Use Ridge or Lasso regression\n",
        "3. Principal Component Analysis (PCA)\n",
        "4. Combine correlated variables into composite variables\n",
        "5. Increase sample size\n",
        "6. Use Variance Inflation Factor (VIF) to identify problematic variables\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13. What are some common techniques for transforming categorical variables for use in regression models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nCommon techniques for categorical variables:\\n1. One-Hot Encoding: Create binary variables for each category\\n2. Label Encoding: Assign numerical values to categories\\n3. Dummy Variables: Use n-1 binary variables for n categories\\n4. Target Encoding: Replace categories with target variable statistics\\n5. Binary Encoding: Convert categories to binary representation\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Common techniques for categorical variables:\n",
        "1. One-Hot Encoding: Create binary variables for each category\n",
        "2. Label Encoding: Assign numerical values to categories\n",
        "3. Dummy Variables: Use n-1 binary variables for n categories\n",
        "4. Target Encoding: Replace categories with target variable statistics\n",
        "5. Binary Encoding: Convert categories to binary representation\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14. What is the role of interaction terms in Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nInteraction terms capture the combined effect of two or more variables on the\\ndependent variable. They allow the effect of one variable to depend on the\\nvalue of another variable. Example: Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁×X₂)\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Interaction terms capture the combined effect of two or more variables on the\n",
        "dependent variable. They allow the effect of one variable to depend on the\n",
        "value of another variable. Example: Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁×X₂)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nSimple Linear Regression: Intercept is Y when X = 0\\nMultiple Linear Regression: Intercept is Y when ALL independent variables = 0\\nIn multiple regression, the intercept often has less practical meaning since\\nhaving all variables equal zero may not be realistic.\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Simple Linear Regression: Intercept is Y when X = 0\n",
        "Multiple Linear Regression: Intercept is Y when ALL independent variables = 0\n",
        "In multiple regression, the intercept often has less practical meaning since\n",
        "having all variables equal zero may not be realistic.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe slope represents the rate of change in the dependent variable for each\\nunit change in the independent variable. It directly affects predictions:\\n- Larger absolute slope = greater impact on predictions\\n- Positive slope = predictions increase with X\\n- Negative slope = predictions decrease with X\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The slope represents the rate of change in the dependent variable for each\n",
        "unit change in the independent variable. It directly affects predictions:\n",
        "- Larger absolute slope = greater impact on predictions\n",
        "- Positive slope = predictions increase with X\n",
        "- Negative slope = predictions decrease with X\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17. How does the intercept in a regression model provide context for the relationship between variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe intercept provides a baseline value and context:\\n- Shows the expected value of Y when predictors are at their reference point\\n- Helps understand the practical range of the relationship\\n- Can indicate whether the relationship makes logical sense\\n- Important for interpreting the overall model fit\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The intercept provides a baseline value and context:\n",
        "- Shows the expected value of Y when predictors are at their reference point\n",
        "- Helps understand the practical range of the relationship\n",
        "- Can indicate whether the relationship makes logical sense\n",
        "- Important for interpreting the overall model fit\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18. What are the limitations of using R² as a sole measure of model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nLimitations of R²:\\n1. Doesn't indicate if model assumptions are met\\n2. Can be artificially inflated by adding more variables\\n3. Doesn't show if relationships are causal\\n4. High R² doesn't guarantee good predictions on new data\\n5. Doesn't detect overfitting\\n6. May not be appropriate for non-linear relationships\\n\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Limitations of R²:\n",
        "1. Doesn't indicate if model assumptions are met\n",
        "2. Can be artificially inflated by adding more variables\n",
        "3. Doesn't show if relationships are causal\n",
        "4. High R² doesn't guarantee good predictions on new data\n",
        "5. Doesn't detect overfitting\n",
        "6. May not be appropriate for non-linear relationships\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19. How would you interpret a large standard error for a regression coefficient?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLarge standard error indicates:\\n- High uncertainty in the coefficient estimate\\n- Coefficient may not be statistically significant\\n- Wide confidence intervals for the coefficient\\n- Potential issues with multicollinearity or insufficient data\\n- Less reliable predictions based on that variable\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Large standard error indicates:\n",
        "- High uncertainty in the coefficient estimate\n",
        "- Coefficient may not be statistically significant\n",
        "- Wide confidence intervals for the coefficient\n",
        "- Potential issues with multicollinearity or insufficient data\n",
        "- Less reliable predictions based on that variable\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nIdentification: Plot residuals vs fitted values. Heteroscedasticity shows as:\\n- Fan-shaped or funnel pattern\\n- Increasing/decreasing variance with fitted values\\n\\nImportance to address:\\n- Violates regression assumptions\\n- Makes standard errors unreliable\\n- Affects hypothesis testing\\n- Can be fixed with transformations or weighted regression\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Identification: Plot residuals vs fitted values. Heteroscedasticity shows as:\n",
        "- Fan-shaped or funnel pattern\n",
        "- Increasing/decreasing variance with fitted values\n",
        "\n",
        "Importance to address:\n",
        "- Violates regression assumptions\n",
        "- Makes standard errors unreliable\n",
        "- Affects hypothesis testing\n",
        "- Can be fixed with transformations or weighted regression\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nThis indicates:\\n- Model may be overfitted with too many variables\\n- Some variables don't significantly improve the model\\n- Adjusted R² penalizes for additional variables\\n- Need to remove non-significant predictors\\n- Model may not generalize well to new data\\n\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "This indicates:\n",
        "- Model may be overfitted with too many variables\n",
        "- Some variables don't significantly improve the model\n",
        "- Adjusted R² penalizes for additional variables\n",
        "- Need to remove non-significant predictors\n",
        "- Model may not generalize well to new data\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22. Why is it important to scale variables in Multiple Linear Regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nVariable scaling is important because:\\n1. Variables with larger scales can dominate the model\\n2. Coefficients become comparable across variables\\n3. Improves numerical stability of algorithms\\n4. Essential for regularization techniques (Ridge/Lasso)\\n5. Helps with interpretation when variables have different units\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Variable scaling is important because:\n",
        "1. Variables with larger scales can dominate the model\n",
        "2. Coefficients become comparable across variables\n",
        "3. Improves numerical stability of algorithms\n",
        "4. Essential for regularization techniques (Ridge/Lasso)\n",
        "5. Helps with interpretation when variables have different units\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23. What is polynomial regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPolynomial regression is a form of regression analysis where the relationship\\nbetween independent and dependent variables is modeled as an nth degree polynomial.\\nIt extends linear regression to capture non-linear relationships.\\n'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Polynomial regression is a form of regression analysis where the relationship\n",
        "between independent and dependent variables is modeled as an nth degree polynomial.\n",
        "It extends linear regression to capture non-linear relationships.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24. How does polynomial regression differ from linear regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nKey differences:\\n- Linear: Y = β₀ + β₁X (straight line)\\n- Polynomial: Y = β₀ + β₁X + β₂X² + ... + βₙXⁿ (curved line)\\n- Polynomial can capture non-linear patterns\\n- Polynomial has higher complexity and risk of overfitting\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Key differences:\n",
        "- Linear: Y = β₀ + β₁X (straight line)\n",
        "- Polynomial: Y = β₀ + β₁X + β₂X² + ... + βₙXⁿ (curved line)\n",
        "- Polynomial can capture non-linear patterns\n",
        "- Polynomial has higher complexity and risk of overfitting\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25. When is polynomial regression used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPolynomial regression is used when:\\n1. Relationship between variables is non-linear\\n2. Scatter plot shows curved patterns\\n3. Linear regression gives poor fit\\n4. Domain knowledge suggests polynomial relationship\\n5. Need to model growth rates, decay, or cyclical patterns\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Polynomial regression is used when:\n",
        "1. Relationship between variables is non-linear\n",
        "2. Scatter plot shows curved patterns\n",
        "3. Linear regression gives poor fit\n",
        "4. Domain knowledge suggests polynomial relationship\n",
        "5. Need to model growth rates, decay, or cyclical patterns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26. What is the general equation for polynomial regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# General equation for polynomial regression of degree n:\n",
        "# Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε\n",
        "# Where n is the degree of the polynomial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27. Can polynomial regression be applied to multiple variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nYes, polynomial regression can be applied to multiple variables:\\nY = β₀ + β₁X₁ + β₂X₂ + β₃X₁² + β₄X₂² + β₅X₁X₂ + ...\\n\\nThis includes:\\n- Powers of individual variables\\n- Interaction terms between variables\\n- Cross-products of different degrees\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Yes, polynomial regression can be applied to multiple variables:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + β₃X₁² + β₄X₂² + β₅X₁X₂ + ...\n",
        "\n",
        "This includes:\n",
        "- Powers of individual variables\n",
        "- Interaction terms between variables\n",
        "- Cross-products of different degrees\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28. What are the limitations of polynomial regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLimitations:\\n1. Prone to overfitting with high degrees\\n2. Sensitive to outliers\\n3. Poor extrapolation beyond data range\\n4. Increased computational complexity\\n5. May not have physical interpretation\\n6. Requires careful degree selection\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Limitations:\n",
        "1. Prone to overfitting with high degrees\n",
        "2. Sensitive to outliers\n",
        "3. Poor extrapolation beyond data range\n",
        "4. Increased computational complexity\n",
        "5. May not have physical interpretation\n",
        "6. Requires careful degree selection\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nMethods for degree selection:\\n1. Cross-validation\\n2. Information criteria (AIC, BIC)\\n3. Adjusted R²\\n4. Learning curves\\n5. Validation set performance\\n6. F-tests for nested models\\n7. Regularization techniques\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Methods for degree selection:\n",
        "1. Cross-validation\n",
        "2. Information criteria (AIC, BIC)\n",
        "3. Adjusted R²\n",
        "4. Learning curves\n",
        "5. Validation set performance\n",
        "6. F-tests for nested models\n",
        "7. Regularization techniques\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q30. Why is visualization important in polynomial regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nVisualization is crucial because:\\n1. Helps identify non-linear patterns in data\\n2. Shows if polynomial degree is appropriate\\n3. Reveals overfitting (too complex curves)\\n4. Helps detect outliers affecting the fit\\n5. Validates model assumptions\\n6. Assists in communicating results\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Visualization is crucial because:\n",
        "1. Helps identify non-linear patterns in data\n",
        "2. Shows if polynomial degree is appropriate\n",
        "3. Reveals overfitting (too complex curves)\n",
        "4. Helps detect outliers affecting the fit\n",
        "5. Validates model assumptions\n",
        "6. Assists in communicating results\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q31. How is polynomial regression implemented in Python?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nImplementation approaches:\\n1. Using sklearn.preprocessing.PolynomialFeatures + LinearRegression\\n2. Using numpy.polyfit() for simple cases\\n3. Manual feature engineering with pandas\\n4. Using sklearn.pipeline for streamlined workflow\\n\\nExample:\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.linear_model import LinearRegression\\n\\npoly_features = PolynomialFeatures(degree=2)\\nX_poly = poly_features.fit_transform(X)\\nmodel = LinearRegression().fit(X_poly, y)\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Implementation approaches:\n",
        "1. Using sklearn.preprocessing.PolynomialFeatures + LinearRegression\n",
        "2. Using numpy.polyfit() for simple cases\n",
        "3. Manual feature engineering with pandas\n",
        "4. Using sklearn.pipeline for streamlined workflow\n",
        "\n",
        "Example:\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

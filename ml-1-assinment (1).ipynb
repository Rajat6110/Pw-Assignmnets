{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Q&A Study Guide\n",
    "\n",
    "This notebook contains essential machine learning concepts with explanations and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter is a variable in a mathematical or statistical model whose value is estimated from data.\n",
    "# In machine learning models, parameters are the internal variables that the algorithm adjusts during training.\n",
    "# For example, in linear regression, parameters are the coefficients (weights) and the intercept.\n",
    "# These values are optimized using data to minimize errors between predictions and actual results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation is a statistical measure that quantifies the degree to which two variables move in relation to each other.\n",
    "# It ranges from -1 (perfect negative relationship) to +1 (perfect positive relationship), with 0 indicating no linear relationship.\n",
    "# Correlation helps to identify whether and how strongly pairs of variables are related, but it does not imply causation.\n",
    "# Commonly used correlation coefficients include Pearson, Spearman, and Kendall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative correlation means that as one variable increases, the other decreases.\n",
    "# The correlation coefficient value will be less than 0 (e.g., -0.8).\n",
    "# For example, for the relationship between the amount of time spent watching TV and physical activity level, as TV time increases, physical activity typically decreases.\n",
    "# Negative correlation does not necessarily mean causation; it only indicates an inverse relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning (ML) is a subfield of artificial intelligence in which algorithms learn patterns from data to make predictions or decisions.\n",
    "# ML models automatically improve their performance as they are exposed to more data over time, without being explicitly programmed.\n",
    "# Main Components:\n",
    "# 1. Data: The input collected for training and evaluation.\n",
    "# 2. Features: Measurable properties or variables of data (attributes used for modeling).\n",
    "# 3. Model: The mathematical representation for learning patterns from data.\n",
    "# 4. Loss Function: Measures how well a model's predictions match actual values.\n",
    "# 5. Optimizer: The algorithm used to update model parameters to minimize the loss (e.g., Gradient Descent).\n",
    "# 6. Training: The process of adjusting the model parameters using data and the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss value in a machine learning model quantitatively measures the difference between the actual target values and the model's predicted values.\n",
    "# Lower loss values indicate a model that is making more accurate predictions.\n",
    "# A high loss signals the model still makes large errors and may require further training, more data, or a better algorithm.\n",
    "# Continued monitoring of the loss function during training helps to determine if the model is improving or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables are numeric variables that can take an infinite number of values within a range (e.g., height, weight, temperature).\n",
    "# They are often measured on an interval or ratio scale.\n",
    "# Categorical variables represent distinct groups or categories (e.g., color, gender, country).\n",
    "# These are usually measured on a nominal or ordinal scale and may need to be encoded before being used in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most machine learning algorithms require input features to be numeric.\n",
    "# To handle categorical variables, we transform them into numerical representations.\n",
    "# Common techniques:\n",
    "# 1. Label Encoding: Assigns a unique integer to each category (suitable for ordinal data).\n",
    "# 2. One-hot Encoding: Creates binary columns for each unique category (suitable for nominal data).\n",
    "# 3. Ordinal Encoding: Similar to label encoding but takes into account feature hierarchy.\n",
    "# The choice of technique depends on the nature of the categorical variable and the algorithms used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a dataset means exposing a machine learning model to a set of data for learning patterns.\n",
    "# The training set contains input data and the associated correct outputs (labels).\n",
    "# Testing a dataset refers to evaluating the trained model with new, unseen data to check how well it generalizes.\n",
    "# This process helps to estimate the true performance of the model when deployed in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.preprocessing is a module within scikit-learn that offers tools and functions for data preprocessing.\n",
    "# It includes functions for scaling numerical features, normalizing data, encoding categorical features (e.g., LabelEncoder, OneHotEncoder), and transforming features (e.g., PolynomialFeatures).\n",
    "# Preprocessing ensures that data is properly formatted and scaled for use in machine learning models, which can improve performance and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What is a Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test set is the subset of the entire dataset that is held back and not used during the training phase.\n",
    "# After a model is trained, the test set evaluates its predictive performance.\n",
    "# The results on the test set help to estimate how well the model will perform on truly unseen data (in production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split from sklearn.model_selection to randomly split data into training and testing sets.\n",
    "# Example usage:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Here, 20% of the data is reserved for testing, and 80% is used for training.\n",
    "# This process prevents data leakage and ensures an unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The typical steps to solve a machine learning problem are:\n",
    "# 1. Problem Definition: Clearly define the objective and determine what to predict.\n",
    "# 2. Data Collection: Gather the relevant and sufficient data.\n",
    "# 3. Data Cleaning: Handle missing values, remove duplicates, and correct errors.\n",
    "# 4. Exploratory Data Analysis (EDA): Investigate the data to understand distributions, relationships, and anomalies.\n",
    "# 5. Feature Engineering: Select, create, or transform features that help improve model performance.\n",
    "# 6. Model Selection: Choose suitable algorithms based on the problem type and data characteristics.\n",
    "# 7. Training: Fit the model to the training data.\n",
    "# 8. Evaluation: Use metrics to evaluate model performance on test or validation data.\n",
    "# 9. Hyperparameter Tuning: Optimize algorithm settings to improve results.\n",
    "# 10. Deployment: Integrate the model into practical applications, monitor, and retrain as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA) is a crucial step that involves visualizing and summarizing data before modeling.\n",
    "# EDA helps identify patterns, trends, outliers, missing values, and relationships among variables.\n",
    "# It aids in uncovering errors or anomalies that could affect modeling results.\n",
    "# By understanding the data thoroughly, we can make better choices about feature engineering, model selection, and parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation is a numerical measure that describes how two variables move in relation to one another.\n",
    "# Positive correlation means they increase or decrease together, while negative correlation means one increases as the other decreases.\n",
    "# Correlation is commonly used for initial data analysis, feature selection, and understanding variable relationships in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
    "# It is measured by a correlation coefficient between 0 and -1 (e.g., -0.5).\n",
    "# Example: As outdoor temperature falls, the demand for heating likely rises, showing a negative relationship.\n",
    "# Negative correlation does not mean that one variable causes the decrease, only that their values tend to move in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation in Python is commonly calculated using the pandas library with the .corr() method.\n",
    "# Example:\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({...}) # your data\n",
    "# correlation_matrix = df.corr()\n",
    "# This matrix shows the pairwise correlation coefficients for the columns in the DataFrame.\n",
    "# For a single pair, use df['col1'].corr(df['col2'])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causation means one variable directly affects or brings about a change in another variable.\n",
    "# Correlation, on the other hand, simply implies that two variables have a relationship, but one does not necessarily cause the other.\n",
    "# For example: Ice cream sales and drowning deaths are correlated (both rise in summer), but eating ice cream does not cause drowningâ€”summer is the confounding cause.\n",
    "# Therefore: Correlation does not imply causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optimizer is an algorithm or method used to adjust the parameters of a machine learning model during training to minimize the loss function.\n",
    "# It iteratively updates weights based on gradients computed from the loss.\n",
    "# Common optimizers:\n",
    "# 1. Gradient Descent: Moves parameters in the direction of steepest loss reduction using the gradient.\n",
    "#    Example: optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# 2. Adam (Adaptive Moment Estimation): Maintains adaptive learning rates by using moving averages of gradient and squared gradient.\n",
    "#    Example: optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# 3. RMSProp: Adapts learning rates based on a decaying average of squared gradients, often used for recurrent neural networks.\n",
    "# Choice of optimizer can impact training speed and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. What is sklearn.linear_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.linear_model is a module in Python's scikit-learn library that contains linear models for regression and classification tasks.\n",
    "# Examples include LinearRegression, LogisticRegression, Ridge, and Lasso.\n",
    "# These models are based on linear equations that relate features to targets and are used for problems where a linear relationship is assumed.\n",
    "# The module provides functions for fitting, predicting, and evaluating linear models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit() is a method used to train a machine learning model on input features (X) and the corresponding target outputs (y).\n",
    "# It adjusts the model parameters to best map inputs to outputs using the specified learning algorithm.\n",
    "# Arguments:\n",
    "# - X: The training input data (features)\n",
    "# - y: The known labels (targets)\n",
    "# Examples:\n",
    "# model.fit(X_train, y_train)\n",
    "# For unsupervised models, y may not be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict() is used to make predictions on new, unseen data using a trained model.\n",
    "# It takes input features (X), applies learned parameters, and returns predicted labels or values.\n",
    "# Arguments:\n",
    "# - X: The feature array or dataset for which predictions are required.\n",
    "# Example:\n",
    "# predictions = model.predict(X_test)\n",
    "# The result can be used to evaluate model performance or for practical decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables are numeric and can take on any value within a range, including decimals (e.g., height, weight, age).\n",
    "# They are useful for regression problems and are measured, not counted.\n",
    "# Categorical variables consist of discrete categories, labels, or groups.\n",
    "# Examples include eye color (blue, green, brown), city names, or binary outcomes (yes/no).\n",
    "# Categorical variables often need to be encoded numerically for use in ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling is the process of normalizing or standardizing the range of independent variables or features of data.\n",
    "# It ensures that each feature contributes equally to the model, preventing features with larger value ranges from dominating.\n",
    "# Common methods include:\n",
    "# - Min-Max Scaling: Scales features to a fixed range (usually 0 to 1).\n",
    "# - Standardization (Z-score): Centers data around zero with a standard deviation of one.\n",
    "# Scaling is critical for algorithms that calculate distances (KNN, SVM) or gradients (neural networks), as it speeds up convergence and may improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, scaling is typically performed using scikit-learn's preprocessing module.\n",
    "# StandardScaler performs Z-score standardization; MinMaxScaler transforms data to a fixed range.\n",
    "# Example using StandardScaler:\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# For Min-Max scaling:\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Explain data encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data encoding is the process of converting categorical variables into a numerical format that can be provided to machine learning algorithms.\n",
    "# Many models require inputs to be integers or floats; encoding makes data machine-readable.\n",
    "# Common encoding types:\n",
    "# - Label Encoding: Converts each category to a unique integer. Useful for ordinal features.\n",
    "# - One-hot Encoding: For each category, creates a separate binary column (0 or 1). Useful for nominal features.\n",
    "# Encoding is necessary for algorithms like SVM, linear regression, and neural networks which cannot handle categorical text data directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}